{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Major Project Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mjUrAVXrpTaJ62sY9A_s1cFZcHXpfc8w",
      "authorship_tag": "ABX9TyOApT/dI8yA7256MspeW27n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghvender1205/SentimentAnalysis_MajorProject/blob/master/Major_Project_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGvEHDaXHNyR"
      },
      "source": [
        "# Major Project SmartKnower Internship\n",
        "\n",
        "## Problem Statement\n",
        "Sentiment Analysis use ML Based Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpZQilorF-vF",
        "outputId": "ec91d36d-ac86-4d6a-ca79-1db349d6768a"
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # vectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC # Supported Vector Machine\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score # metrics\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "from textblob import TextBlob, Word\n",
        "from bs4 import BeautifulSoup\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(os.listdir('/content/drive/MyDrive/SmartKnower/MajorProject'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Major Project Sentiment Analysis.ipynb', 'IMDB Dataset.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fkifBeeJhGH"
      },
      "source": [
        "## Data\n",
        "\n",
        "I am using IMDB 50K Movie Reviews Dataset, from Kaggle Website\n",
        "\n",
        "Link: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoEsURnuHtth"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/SmartKnower/MajorProject/IMDB Dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "LcnGYKsBJ2SI",
        "outputId": "70fb6680-9d75-4582-ec74-cb33e1029a65"
      },
      "source": [
        "print(data.shape)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gOoHwe3KGoO"
      },
      "source": [
        "### EDA on the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "zkMF_EX-J7VF",
        "outputId": "c972e56f-6629-41e8-ae03-ee660dfe2399"
      },
      "source": [
        "# Summary\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  positive\n",
              "freq                                                    5     25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqs5u6zIKOH-",
        "outputId": "e62bc5e9-6674-445d-faa0-c114100a3311"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ3LammrKeTu",
        "outputId": "f7966c49-46fc-495b-f5c6-afed4698581a"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['review', 'sentiment'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoD2JwOFKqe_"
      },
      "source": [
        "### Sentiment Count\n",
        "The number of occurrences of positive and negative words in each document was counted to determine the document's sentiment score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmCIUKIcKhkL",
        "outputId": "b803372e-e9fb-4bb9-e719-9fdd1d633528"
      },
      "source": [
        "data['sentiment'].value_counts() # Balanced dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-VpMwtkLGi5"
      },
      "source": [
        "### Split Dataset\n",
        "\n",
        "Split the Dataset into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXMgFL2yLBus",
        "outputId": "f05d6af0-70e2-4823-cdf2-b0cb2fc90708"
      },
      "source": [
        "# Train\n",
        "train_reviews = data.review[:40000]\n",
        "train_sentiments = data.sentiment[:40000]\n",
        "\n",
        "# Test\n",
        "test_reviews = data.review[40000:]\n",
        "test_sentiments = data.sentiment[40000:]\n",
        "\n",
        "print(train_reviews.shape, train_sentiments.shape)\n",
        "print(test_reviews.shape, test_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000,) (40000,)\n",
            "(10000,) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q-5452gLqnq"
      },
      "source": [
        "### Text Normalization\n",
        "\n",
        "Text normalization is the process of transforming text into a single canonical form that it might not have had before.\n",
        "\n",
        "\n",
        "Make sure to download NLTK Stopwords first\n",
        "```python\n",
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9d2t8HlMn6b",
        "outputId": "cb5f95ce-8195-4773-ed1c-22aa28a8e05f"
      },
      "source": [
        "%%bash\n",
        "python\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkqFrHF3LpC2"
      },
      "source": [
        "# Tokenization of Text\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "# Setting English stopwords\n",
        "stopword_list = nltk.corpus.stopwords.words('english') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8XC2Ew2NAGm"
      },
      "source": [
        "### Removing html strips and noise text\n",
        "\n",
        "Removing html strips like eg: ```\"<b></b>\"``` tags and some regex text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRqgvlr7MLbz"
      },
      "source": [
        "def strip_html(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  return soup.get_text()\n",
        "\n",
        "# Removing the square brackets\n",
        "def remove_brackets(text):\n",
        "  return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "\n",
        "# Removing the noisy text\n",
        "def denoise_text(text):\n",
        "  text = strip_html(text)\n",
        "  text = remove_brackets(text)\n",
        "  return text\n",
        "\n",
        "# Apply function on the dataset\n",
        "data['review'] = data['review'].apply(denoise_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "Ijm6N648OD3u",
        "outputId": "4644ab94-d21d-44c9-b6d9-755941d634d4"
      },
      "source": [
        "data.head(10) # See the difference from above"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. The filming tec...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. The filming tec...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5  Probably my all-time favorite movie, a story o...  positive\n",
              "6  I sure would like to see a resurrection of a u...  positive\n",
              "7  This show was an amazing, fresh & innovative i...  negative\n",
              "8  Encouraged by the positive comments about this...  negative\n",
              "9  If you like original gut wrenching laughter yo...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh3B4pewOOAM"
      },
      "source": [
        "### Removing Some Special Characters\n",
        "\n",
        "Removing some special characters like regex expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4MSh5vvOIu-"
      },
      "source": [
        "def remove_special_char(text, remove_digits=True):\n",
        "  pattern = r'[^a-zA-z0-9\\s]'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text\n",
        "\n",
        "# Apply\n",
        "data['review'] = data['review'].apply(remove_special_char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "u3jNP8bUOrib",
        "outputId": "d177cb40-f781-4bdf-effe-b6ea6f076a62"
      },
      "source": [
        "data.head(10) # Difference...!!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production The filming tech...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically theres a family where a little boy J...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Matteis Love in the Time of Money is a ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my alltime favorite movie a story of ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing fresh  innovative ide...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production The filming tech...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically theres a family where a little boy J...  negative\n",
              "4  Petter Matteis Love in the Time of Money is a ...  positive\n",
              "5  Probably my alltime favorite movie a story of ...  positive\n",
              "6  I sure would like to see a resurrection of a u...  positive\n",
              "7  This show was an amazing fresh  innovative ide...  negative\n",
              "8  Encouraged by the positive comments about this...  negative\n",
              "9  If you like original gut wrenching laughter yo...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1endckCWOzD0"
      },
      "source": [
        "### Text Stemming\n",
        "\n",
        "It is the process of removing inflected or derivative words to their word stem either base or root form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqpknNiCOutg"
      },
      "source": [
        "# Stemming\n",
        "def SimpleStemmer(text):\n",
        "  ps = PorterStemmer()\n",
        "  text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "  return text\n",
        "\n",
        "# Apply \n",
        "data['review'] = data['review'].apply(SimpleStemmer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "968_TzgbQTHJ"
      },
      "source": [
        "### Removing stopwords\n",
        "\n",
        "stop words are words which are filtered out before or after processing of natural language data.\n",
        "\n",
        "Useless words are ```stopwords```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGlc5LPfPi0X",
        "outputId": "cfc4f9e2-20ea-4a9d-fcf1-cd2e2c9e67ac"
      },
      "source": [
        "# Set stopwords to English\n",
        "stop = set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  if is_lower_case:\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "  else:\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    \n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "  \n",
        "# Apply Function\n",
        "data['review'] = data['review'].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a', 'shan', 'now', 'her', \"wouldn't\", 'had', 'his', 'me', 'do', 'but', \"that'll\", 'were', 'down', \"you'd\", 'theirs', 'before', 'been', 's', 'mustn', 'just', 'after', \"she's\", 'was', 'above', 'below', \"don't\", 'who', 'needn', 'are', 'your', 'such', 'itself', 'my', 'can', \"didn't\", 'them', 'then', 'hers', 'didn', \"shan't\", 'wasn', 'these', 'she', 'into', 'so', 'am', 'having', 'while', 'why', 'shouldn', \"it's\", 'themselves', 'by', 'between', 'both', 'doesn', 'too', 'did', \"hasn't\", 'isn', \"you're\", 'through', 'those', 'where', 'm', 'is', 'o', 'aren', \"needn't\", \"haven't\", 'when', \"shouldn't\", 'have', 'very', \"weren't\", 'most', 'not', 'i', 'we', 'ain', 'there', 'no', \"should've\", 'should', 'being', 'does', 'few', \"aren't\", 'for', 'won', 'hasn', 'herself', 'from', \"hadn't\", \"mustn't\", 'this', 'during', 'own', \"you'll\", 'y', 'of', 'than', 'that', \"couldn't\", 'd', 'has', 'yourselves', 'himself', 've', 'to', 'hadn', 'ma', 'mightn', 'all', 'he', 'again', 'any', 'only', 'because', 'out', 'they', 'at', 'up', 'more', 'myself', 're', \"wasn't\", 'off', 'on', \"you've\", 'him', 'with', 'here', 'and', 'yourself', 'how', 'same', 'wouldn', 'whom', 'doing', \"won't\", 'couldn', \"doesn't\", 'over', 'it', 'if', 'be', 'some', 'against', 't', 'nor', 'will', 'further', 'its', 'which', 'the', 'under', 'each', 'an', 'haven', 'you', 'other', 'once', 'yours', 'don', 'in', \"mightn't\", 'their', 'weren', \"isn't\", 'as', 'about', 'll', 'what', 'ours', 'ourselves', 'our', 'or', 'until'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1vn6vsjY2GU"
      },
      "source": [
        "### Normalized Train and Test Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "CdW0OtZkYo2m",
        "outputId": "eb2ef133-6406-4b8b-edb5-5903051eba31"
      },
      "source": [
        "# Normalized Train Reviews\n",
        "norm_train_reviews = data.review[:40000] \n",
        "norm_train_reviews[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'one review ha mention watch 1 Oz episod youll hook right thi exactli happen meth first thing struck Oz wa brutal unflinch scene violenc set right word GO trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call OZ nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda Em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast Oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch Oz may becom comfort uncomfort viewingthat get touch darker side'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "SQqG6Ba_Zpa8",
        "outputId": "b28552db-9ffc-4fee-ec98-ae238ece1492"
      },
      "source": [
        "# Normalized Test Reviews\n",
        "norm_test_reviews = data.review[40000:]\n",
        "norm_test_reviews[45005]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'read review watch thi piec cinemat garbag took least 2 page find somebodi els didnt think thi appallingli unfunni montag wasnt acm humour 70 inde ani era thi isnt least funni set sketch comedi ive ever seen itll till come along half skit alreadi done infinit better act monti python woodi allen wa say nice piec anim last 90 second highlight thi film would still get close sum mindless drivelridden thi wast 75 minut semin comedi onli world semin realli doe mean semen scatolog humour onli world scat actual fece precursor joke onli mean thi handbook comedi tit bum odd beaver niceif pubesc boy least one hand free havent found playboy exist give break becaus wa earli 70 way sketch comedi go back least ten year prior onli way could even forgiv thi film even made wa gunpoint retro hardli sketch clown subtli pervert children may cut edg circl could actual funni come realli quit sad kept go throughout entir 75 minut sheer belief may save genuin funni skit end gave film 1 becaus wa lower scoreand onli recommend insomniac coma patientsor perhap peopl suffer lockjawtheir jaw would final drop open disbelief'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Wq88_TdYlB"
      },
      "source": [
        "### Bags of Words Model / Vectorization\n",
        "\n",
        "It is a way of extracting features from text for use in modeling.\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "1. A vocabulary of known words\n",
        "2. A measure of the presence of known words\n",
        "\n",
        "In here it is used to convert text documents to numerical vectors or ```bag of words```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik69fFRhdIe7",
        "outputId": "26063318-b7e1-4c1a-ea55-ac030dbd726d"
      },
      "source": [
        "# Count Vectorizer\n",
        "cv = CountVectorizer(min_df=0, max_df=1, binary=False, ngram_range=(1, 3))\n",
        "\n",
        "# Transformed Train and Test Reviews\n",
        "cv_train_reviews = cv.fit_transform(norm_train_reviews)\n",
        "cv_test_reviews = cv.fit_transform(norm_test_reviews)\n",
        "\n",
        "print('BOW_cv_train: ', cv_train_reviews.shape)\n",
        "print('BOW_cv_test: ', cv_test_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW_cv_train:  (40000, 6209089)\n",
            "BOW_cv_test:  (10000, 1828357)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip3PAZtLepuk",
        "outputId": "8957e2d3-a997-47dc-e919-716a20d46751"
      },
      "source": [
        "vocab = cv.get_feature_names() # Get Feature Names\n",
        "\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00 agent',\n",
              " '00 agent difficult',\n",
              " '00 come',\n",
              " '00 come back',\n",
              " '00 includ',\n",
              " '00 includ unusu',\n",
              " '00 product',\n",
              " '00 product compani',\n",
              " '00 schneider',\n",
              " '00 schneider murderervillain',\n",
              " '00 still',\n",
              " '00 still hold',\n",
              " '00 wife',\n",
              " '00 wife usual',\n",
              " '000',\n",
              " '000 produc',\n",
              " '000 produc thi',\n",
              " '00015',\n",
              " '00015 second',\n",
              " '00015 second prod',\n",
              " '001',\n",
              " '001 laughomet',\n",
              " '001 laughomet 1000',\n",
              " '002',\n",
              " '002 hope',\n",
              " '002 hope thi',\n",
              " '007 adventur',\n",
              " '007 adventur much',\n",
              " '007 appear',\n",
              " '007 appear script',\n",
              " '007 atmosphereon',\n",
              " '007 atmosphereon hand',\n",
              " '007 bruce',\n",
              " '007 bruce lee',\n",
              " '007 film',\n",
              " '007 film made',\n",
              " '007 follow',\n",
              " '007 follow ha',\n",
              " '007 franchis',\n",
              " '007 franchis star',\n",
              " '007 frwl',\n",
              " '007 frwl come',\n",
              " '007 gadget',\n",
              " '007 gadget equal',\n",
              " '007 game',\n",
              " '007 game producedfantast',\n",
              " '007 goldeney',\n",
              " '007 goldeney one',\n",
              " '007 look',\n",
              " '007 look like',\n",
              " '007 movi baron',\n",
              " '007 movi eas',\n",
              " '007 movi sean',\n",
              " '007 movi wooden',\n",
              " '007 movieswhat',\n",
              " '007 movieswhat put',\n",
              " '007 music',\n",
              " '007 music theme',\n",
              " '007 scene',\n",
              " '007 scene wipe',\n",
              " '007 theme',\n",
              " '007 theme kind',\n",
              " '0079',\n",
              " '0079 time',\n",
              " '0079 time one',\n",
              " '0080',\n",
              " '0080 0083',\n",
              " '0080 0083 08th',\n",
              " '0083 08th',\n",
              " '0083 08th ms',\n",
              " '0083 stardust',\n",
              " '0083 stardust memori',\n",
              " '01 budget',\n",
              " '01 budget excit',\n",
              " '01 may',\n",
              " '01 may relev',\n",
              " '01 wa',\n",
              " '01 wa possibl',\n",
              " '010 avoid',\n",
              " '010 avoid cost',\n",
              " '010 extrem',\n",
              " '010 extrem ridicul',\n",
              " '010 film',\n",
              " '010 film doe',\n",
              " '010 mayb',\n",
              " '010 mayb neg',\n",
              " '010 onli',\n",
              " '010 onli vote',\n",
              " '010 option',\n",
              " '010 option otherwis',\n",
              " '010 per',\n",
              " '010 per hour',\n",
              " '010 plot',\n",
              " '010 plot lol10',\n",
              " '010 star',\n",
              " '0101',\n",
              " '0101 jewish',\n",
              " '0101 jewish guy',\n",
              " '02 10',\n",
              " '02 wa',\n",
              " '02 wa onli',\n",
              " '0205',\n",
              " '0205 comedi',\n",
              " '0205 comedi action',\n",
              " '030',\n",
              " '030 100219',\n",
              " '030 100219 end',\n",
              " '03092005',\n",
              " '03092005 mention',\n",
              " '03092005 mention staci',\n",
              " '039',\n",
              " '039 anna',\n",
              " '039 anna christi',\n",
              " '04',\n",
              " '04 support',\n",
              " '04 support procreat',\n",
              " '044',\n",
              " '044 big',\n",
              " '044 big trail',\n",
              " '048',\n",
              " '048 mile',\n",
              " '048 mile uncl',\n",
              " '05 fact',\n",
              " '05 fact shorter',\n",
              " '05 love',\n",
              " '05 love go',\n",
              " '05 maci',\n",
              " '05 maci moor',\n",
              " '06 even',\n",
              " '06 even know',\n",
              " '06 follow',\n",
              " '06 follow discuss',\n",
              " '06 tough',\n",
              " '06 tough task',\n",
              " '0615',\n",
              " '0615 never',\n",
              " '0615 never less',\n",
              " '06and',\n",
              " '06and realli',\n",
              " '06and realli love',\n",
              " '07kiloton',\n",
              " '07kiloton missil',\n",
              " '07kiloton missil love',\n",
              " '08 either',\n",
              " '08 either way',\n",
              " '08 movi',\n",
              " '08 movi gunshot',\n",
              " '081006',\n",
              " '081006 us',\n",
              " '081006 us watch',\n",
              " '09',\n",
              " '09 star',\n",
              " '0f 10 thi',\n",
              " '0f mutini',\n",
              " '0f mutini bounti',\n",
              " '0s add',\n",
              " '0s add comput',\n",
              " '0s got',\n",
              " '0s got vote',\n",
              " '0s think',\n",
              " '0s think sake',\n",
              " '10 10 10',\n",
              " '10 10 84',\n",
              " '10 10 amateurish',\n",
              " '10 10 beer',\n",
              " '10 10 believ',\n",
              " '10 10 charact',\n",
              " '10 10 despit',\n",
              " '10 10 go',\n",
              " '10 10 got',\n",
              " '10 10 grandpar',\n",
              " '10 10 highli',\n",
              " '10 10 invent',\n",
              " '10 10 ive',\n",
              " '10 10 point',\n",
              " '10 10 rate',\n",
              " '10 10 score',\n",
              " '10 10 sorri',\n",
              " '10 10 star',\n",
              " '10 10 starsprob',\n",
              " '10 10 suppos',\n",
              " '10 10 thank',\n",
              " '10 10 true',\n",
              " '10 10 wa',\n",
              " '10 10 whi',\n",
              " '10 10 would',\n",
              " '10 10arjun',\n",
              " '10 10highli',\n",
              " '10 10highli highli',\n",
              " '10 10p',\n",
              " '10 10p admit',\n",
              " '10 12',\n",
              " '10 12 year',\n",
              " '10 15 minut',\n",
              " '10 15 rang',\n",
              " '10 15year',\n",
              " '10 15year date',\n",
              " '10 17',\n",
              " '10 17 year',\n",
              " '10 18',\n",
              " '10 18 age',\n",
              " '10 1969',\n",
              " '10 1969 thi',\n",
              " '10 20 second',\n",
              " '10 20 year',\n",
              " '10 2001',\n",
              " '10 2001 id',\n",
              " '10 50',\n",
              " '10 50 day',\n",
              " '10 84',\n",
              " '10 84 rate',\n",
              " '10 abl',\n",
              " '10 abl mark',\n",
              " '10 acquir',\n",
              " '10 acquir dvd',\n",
              " '10 actual possibl',\n",
              " '10 actual wa',\n",
              " '10 also',\n",
              " '10 also thought',\n",
              " '10 amateur',\n",
              " '10 amateur could',\n",
              " '10 amateurish',\n",
              " '10 amateurish mess',\n",
              " '10 amaz',\n",
              " '10 amaz movi',\n",
              " '10 among',\n",
              " '10 among actioncomedi',\n",
              " '10 anyon',\n",
              " '10 anyon care',\n",
              " '10 au',\n",
              " '10 au suivant',\n",
              " '10 avoid',\n",
              " '10 avoid ani',\n",
              " '10 bare',\n",
              " '10 base',\n",
              " '10 base qualiti',\n",
              " '10 basi',\n",
              " '10 basi flip',\n",
              " '10 bc',\n",
              " '10 bc like',\n",
              " '10 beauti',\n",
              " '10 beauti ksm',\n",
              " '10 becaus act',\n",
              " '10 becaus actor',\n",
              " '10 becaus click',\n",
              " '10 becaus doesnt',\n",
              " '10 becaus everyth',\n",
              " '10 becaus expos',\n",
              " '10 becaus filmographi',\n",
              " '10 becaus give',\n",
              " '10 becaus im',\n",
              " '10 becaus imdb',\n",
              " '10 becaus keep',\n",
              " '10 becaus littl',\n",
              " '10 becaus love',\n",
              " '10 becaus lowest',\n",
              " '10 becaus movi',\n",
              " '10 becaus noth',\n",
              " '10 becaus onli',\n",
              " '10 becaus pari',\n",
              " '10 becaus song',\n",
              " '10 becaus still',\n",
              " '10 becaus stori',\n",
              " '10 becaus villain',\n",
              " '10 becaus wa',\n",
              " '10 becaus zero',\n",
              " '10 beer',\n",
              " '10 believ',\n",
              " '10 believ user',\n",
              " '10 best music',\n",
              " '10 bestno',\n",
              " '10 bestno small',\n",
              " '10 bo',\n",
              " '10 bo reput',\n",
              " '10 book compar',\n",
              " '10 book thi',\n",
              " '10 book whi',\n",
              " '10 buck sappi',\n",
              " '10 buck watch',\n",
              " '10 buy',\n",
              " '10 buy friend',\n",
              " '10 camera',\n",
              " '10 camera film',\n",
              " '10 cannot',\n",
              " '10 cannot object',\n",
              " '10 cant',\n",
              " '10 cant think',\n",
              " '10 car',\n",
              " '10 car pileup',\n",
              " '10 cent',\n",
              " '10 cent dont',\n",
              " '10 channel',\n",
              " '10 channel uk',\n",
              " '10 charact',\n",
              " '10 charact beauti',\n",
              " '10 collect',\n",
              " '10 collect film',\n",
              " '10 come',\n",
              " '10 come hatchet',\n",
              " '10 comedi',\n",
              " '10 comedi ever',\n",
              " '10 command definit',\n",
              " '10 command sean',\n",
              " '10 comparison',\n",
              " '10 comparison movi',\n",
              " '10 convent',\n",
              " '10 convent wisdom',\n",
              " '10 could give',\n",
              " '10 could half',\n",
              " '10 could otherwis',\n",
              " '10 could turn',\n",
              " '10 creat',\n",
              " '10 creat media',\n",
              " '10 creepi',\n",
              " '10 creepi scari',\n",
              " '10 daughter',\n",
              " '10 daughter 45',\n",
              " '10 day edtv',\n",
              " '10 day rental',\n",
              " '10 derang',\n",
              " '10 derang one',\n",
              " '10 deserv',\n",
              " '10 deserv noth',\n",
              " '10 despit hi',\n",
              " '10 despit submiss',\n",
              " '10 differ stori',\n",
              " '10 differ unfinish',\n",
              " '10 dig',\n",
              " '10 dig somewher',\n",
              " '10 dino',\n",
              " '10 dino bravo',\n",
              " '10 dir',\n",
              " '10 dir billi',\n",
              " '10 dirmark',\n",
              " '10 dirmark dindal',\n",
              " '10 doesnt',\n",
              " '10 doesnt get',\n",
              " '10 dollar',\n",
              " '10 dollar quad',\n",
              " '10 dollars23',\n",
              " '10 dollars23 way',\n",
              " '10 drive',\n",
              " '10 drive sequenc',\n",
              " '10 due',\n",
              " '10 due effort',\n",
              " '10 dvd',\n",
              " '10 dvd rental',\n",
              " '10 eleph',\n",
              " '10 eleph shown',\n",
              " '10 end',\n",
              " '10 end lower',\n",
              " '10 entertain falcon',\n",
              " '10 episod disappoint',\n",
              " '10 episod macgyv',\n",
              " '10 episodesunfortun',\n",
              " '10 episodesunfortun seem',\n",
              " '10 equival',\n",
              " '10 equival letter',\n",
              " '10 even',\n",
              " '10 even wa',\n",
              " '10 excel',\n",
              " '10 excel justin',\n",
              " '10 exclus',\n",
              " '10 exclus graphic',\n",
              " '10 fantast',\n",
              " '10 fantast cant',\n",
              " '10 far',\n",
              " '10 far better',\n",
              " '10 favorit film',\n",
              " '10 favorit horror',\n",
              " '10 favorit slasher',\n",
              " '10 favorit voic',\n",
              " '10 feel',\n",
              " '10 feel free',\n",
              " '10 feet behind',\n",
              " '10 feet indian',\n",
              " '10 feetthen',\n",
              " '10 feetthen run',\n",
              " '10 fgf',\n",
              " '10 fgf godfath',\n",
              " '10 film alltim',\n",
              " '10 film enjoy',\n",
              " '10 film ever',\n",
              " '10 film mayb',\n",
              " '10 final',\n",
              " '10 final disc',\n",
              " '10 first',\n",
              " '10 first second',\n",
              " '10 five',\n",
              " '10 five averag',\n",
              " '10 flood',\n",
              " '10 flood 11',\n",
              " '10 foot',\n",
              " '10 foot pole',\n",
              " '10 footag',\n",
              " '10 footag includ',\n",
              " '10 full',\n",
              " '10 full 10',\n",
              " '10 fun',\n",
              " '10 fun popcorn',\n",
              " '10 game',\n",
              " '10 game period',\n",
              " '10 gave',\n",
              " '10 get',\n",
              " '10 ghouli',\n",
              " '10 ghouli get',\n",
              " '10 give movi',\n",
              " '10 give show',\n",
              " '10 given',\n",
              " '10 given thi',\n",
              " '10 go',\n",
              " '10 go rent',\n",
              " '10 god',\n",
              " '10 good',\n",
              " '10 good sequenc',\n",
              " '10 gorgeou',\n",
              " '10 gorgeou terribl',\n",
              " '10 got see',\n",
              " '10 got thi',\n",
              " '10 grandpar',\n",
              " '10 grandpar like',\n",
              " '10 ha great',\n",
              " '10 ha suspens',\n",
              " '10 havent',\n",
              " '10 havent read',\n",
              " '10 hi',\n",
              " '10 hi song',\n",
              " '10 highli christian',\n",
              " '10 hint',\n",
              " '10 hint unlock',\n",
              " '10 homoerot',\n",
              " '10 homoerot subtext',\n",
              " '10 hour einstein',\n",
              " '10 hour flight',\n",
              " '10 howev',\n",
              " '10 howev realli',\n",
              " '10 im gener',\n",
              " '10 im usual',\n",
              " '10 imdb god',\n",
              " '10 imdb yet',\n",
              " '10 incident',\n",
              " '10 incident ive',\n",
              " '10 independ',\n",
              " '10 independ film',\n",
              " '10 instant',\n",
              " '10 instant flop',\n",
              " '10 instead absolut',\n",
              " '10 instead period',\n",
              " '10 instead weepi',\n",
              " '10 invent',\n",
              " '10 invent use',\n",
              " '10 isol',\n",
              " '10 isol consid',\n",
              " '10 ive given',\n",
              " '10 ive hit',\n",
              " '10 ive long',\n",
              " '10 joke',\n",
              " '10 joke keep',\n",
              " '10 judg ani',\n",
              " '10 judg strength',\n",
              " '10 keep',\n",
              " '10 keep young',\n",
              " '10 kill',\n",
              " '10 kill minor',\n",
              " '10 known',\n",
              " '10 known actor',\n",
              " '10 level',\n",
              " '10 level write',\n",
              " '10 lightli',\n",
              " '10 lightli thi',\n",
              " '10 like',\n",
              " '10 like candi',\n",
              " '10 line becaus',\n",
              " '10 line cant',\n",
              " '10 line comment',\n",
              " '10 line commentari',\n",
              " '10 line demand',\n",
              " '10 line els',\n",
              " '10 line ill',\n",
              " '10 line minium',\n",
              " '10 line soblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblah',\n",
              " '10 line sorri',\n",
              " '10 line think',\n",
              " '10 line total',\n",
              " '10 line watch',\n",
              " '10 lineswellthi',\n",
              " '10 lineswellthi interest',\n",
              " '10 list time',\n",
              " '10 listen',\n",
              " '10 listen peopl',\n",
              " '10 littl entertain',\n",
              " '10 littl high',\n",
              " '10 littl slow',\n",
              " '10 look',\n",
              " '10 look laugh',\n",
              " '10 loss',\n",
              " '10 loss find',\n",
              " '10 low',\n",
              " '10 low budget',\n",
              " '10 main',\n",
              " '10 main reason',\n",
              " '10 mainli',\n",
              " '10 mainli becaus',\n",
              " '10 make better',\n",
              " '10 make pleas',\n",
              " '10 mayb',\n",
              " '10 mayb select',\n",
              " '10 men',\n",
              " '10 men go',\n",
              " '10 midnight',\n",
              " '10 midnight wa',\n",
              " '10 mile',\n",
              " '10 mile jaipur',\n",
              " '10 million amaz',\n",
              " '10 million dollar',\n",
              " '10 million peopl',\n",
              " '10 min disappear',\n",
              " '10 min feel',\n",
              " '10 min handl',\n",
              " '10 min later',\n",
              " '10 minu',\n",
              " '10 minu 12',\n",
              " '10 minut 410',\n",
              " '10 minut becaus',\n",
              " '10 minut bill',\n",
              " '10 minut citizen',\n",
              " '10 minut complet',\n",
              " '10 minut director',\n",
              " '10 minut drop',\n",
              " '10 minut earli',\n",
              " '10 minut emot',\n",
              " '10 minut erect',\n",
              " '10 minut fart',\n",
              " '10 minut father',\n",
              " '10 minut get',\n",
              " '10 minut got',\n",
              " '10 minut guarante',\n",
              " '10 minut happen',\n",
              " '10 minut hit',\n",
              " '10 minut last',\n",
              " '10 minut later',\n",
              " '10 minut like',\n",
              " '10 minut long',\n",
              " '10 minut longin',\n",
              " '10 minut make',\n",
              " '10 minut matt',\n",
              " '10 minut mean',\n",
              " '10 minut middl',\n",
              " '10 minut must',\n",
              " '10 minut noth',\n",
              " '10 minut oh',\n",
              " '10 minut ok',\n",
              " '10 minut parti',\n",
              " '10 minut pass',\n",
              " '10 minut past',\n",
              " '10 minut place',\n",
              " '10 minut prais',\n",
              " '10 minut preced',\n",
              " '10 minut probabl',\n",
              " '10 minut scene',\n",
              " '10 minut suffer',\n",
              " '10 minut switch',\n",
              " '10 minut tell',\n",
              " '10 minut time',\n",
              " '10 minut took',\n",
              " '10 minut truli',\n",
              " '10 minut wa',\n",
              " '10 minut whine',\n",
              " '10 minut wish',\n",
              " '10 minut wonder',\n",
              " '10 minutesi diamond',\n",
              " '10 minutesi dont',\n",
              " '10 minutesruin',\n",
              " '10 minutesruin rest',\n",
              " '10 moron',\n",
              " '10 moron fool',\n",
              " '10 movi good',\n",
              " '10 movi secur',\n",
              " '10 movi shelf',\n",
              " '10 movi shown',\n",
              " '10 movi year',\n",
              " '10 must',\n",
              " '10 must see',\n",
              " '10 new',\n",
              " '10 new speci',\n",
              " '10 nice light',\n",
              " '10 nice sceneri',\n",
              " '10 normal scale',\n",
              " '10 normal would',\n",
              " '10 note',\n",
              " '10 note yet',\n",
              " '10 noth',\n",
              " '10 noth short',\n",
              " '10 occasion',\n",
              " '10 occasion joke',\n",
              " '10 oclock',\n",
              " '10 oclock good',\n",
              " '10 older',\n",
              " '10 older unlik',\n",
              " '10 one peter',\n",
              " '10 one user',\n",
              " '10 one would',\n",
              " '10 onli bc',\n",
              " '10 onli becaus',\n",
              " '10 onli co',\n",
              " '10 onli consist',\n",
              " '10 onli reason',\n",
              " '10 oscar',\n",
              " '10 oscar ten',\n",
              " '10 oyvey',\n",
              " '10 oyvey scale',\n",
              " '10 ozjepp',\n",
              " '10 pack',\n",
              " '10 pack curs',\n",
              " '10 page',\n",
              " '10 page dialoganoth',\n",
              " '10 park',\n",
              " '10 park two',\n",
              " '10 part',\n",
              " '10 part seri',\n",
              " '10 percent',\n",
              " '10 percent right',\n",
              " '10 point fact',\n",
              " '10 point state',\n",
              " '10 pointsfrankli',\n",
              " '10 pointsfrankli thi',\n",
              " '10 probabl closer',\n",
              " '10 probabl deserv',\n",
              " '10 probabl onli',\n",
              " '10 probabl tell',\n",
              " '10 pure',\n",
              " '10 pure bollywood',\n",
              " '10 rate graphic',\n",
              " '10 rate scale',\n",
              " '10 rate system',\n",
              " '10 rate without',\n",
              " '10 reason',\n",
              " '10 reason gamera',\n",
              " '10 rebel',\n",
              " '10 rebel fight',\n",
              " '10 recommend view',\n",
              " '10 recommend watch',\n",
              " '10 regist',\n",
              " '10 regist thi',\n",
              " '10 respect exampl',\n",
              " '10 review',\n",
              " '10 review thi',\n",
              " '10 sadli',\n",
              " '10 sadli offeredwhi',\n",
              " '10 sam',\n",
              " '10 sam mraovich',\n",
              " '10 say',\n",
              " '10 say return',\n",
              " '10 scale',\n",
              " '10 scale thi',\n",
              " '10 scariest',\n",
              " '10 scariest movi',\n",
              " '10 scene',\n",
              " '10 scene never',\n",
              " '10 score',\n",
              " '10 score film',\n",
              " '10 second appear',\n",
              " '10 second edit',\n",
              " '10 second long',\n",
              " '10 second look',\n",
              " '10 second make',\n",
              " '10 second pitch2',\n",
              " '10 second present',\n",
              " '10 second program',\n",
              " '10 second realli',\n",
              " '10 second scene',\n",
              " '10 second shell',\n",
              " '10 second think',\n",
              " '10 seem odd',\n",
              " '10 seem thrown',\n",
              " '10 selfconsci',\n",
              " '10 selfconsci selfabsorb',\n",
              " '10 sever',\n",
              " '10 sever reason',\n",
              " '10 ship',\n",
              " '10 ship extra',\n",
              " '10 short',\n",
              " '10 short movi',\n",
              " '10 show',\n",
              " '10 show odessa',\n",
              " '10 similar',\n",
              " '10 similar situat',\n",
              " '10 sinatra',\n",
              " '10 sinatra truli',\n",
              " '10 sinc kind',\n",
              " '10 sinc thi',\n",
              " '10 slow',\n",
              " '10 slow littl',\n",
              " '10 snooz',\n",
              " '10 snooz tri',\n",
              " '10 someth',\n",
              " '10 someth shall',\n",
              " '10 sometim',\n",
              " '10 sometim music',\n",
              " '10 sorri poor',\n",
              " '10 sorri way',\n",
              " '10 sound right',\n",
              " '10 sound wa',\n",
              " '10 spoiler',\n",
              " '10 spoiler end',\n",
              " '10 sputtosi',\n",
              " '10 sputtosi toronto',\n",
              " '10 squar',\n",
              " '10 squar feet',\n",
              " '10 star becaus',\n",
              " '10 star even',\n",
              " '10 star give',\n",
              " '10 star grade',\n",
              " '10 star michael',\n",
              " '10 star previou',\n",
              " '10 star rate',\n",
              " '10 star take',\n",
              " '10 star thought',\n",
              " '10 star wa',\n",
              " '10 star want',\n",
              " '10 star well',\n",
              " '10 star whi',\n",
              " '10 starsif',\n",
              " '10 starsif want',\n",
              " '10 starsprob',\n",
              " '10 starsprob contain',\n",
              " '10 starsso',\n",
              " '10 starsso sayeth',\n",
              " '10 starsunless',\n",
              " '10 starsunless like',\n",
              " '10 starswhen',\n",
              " '10 starswhen first',\n",
              " '10 still like',\n",
              " '10 still veri',\n",
              " '10 still wont',\n",
              " '10 stori',\n",
              " '10 stori veri',\n",
              " '10 storylin',\n",
              " '10 storylin stuff',\n",
              " '10 sublim',\n",
              " '10 sublim minut',\n",
              " '10 subplot',\n",
              " '10 subplot could',\n",
              " '10 suppos compar',\n",
              " '10 suppos feelgood',\n",
              " '10 synchron',\n",
              " '10 synchron swimmer',\n",
              " '10 technic',\n",
              " '10 technic onli',\n",
              " '10 tempt',\n",
              " '10 tempt lower',\n",
              " '10 ten',\n",
              " '10 ten good',\n",
              " '10 tend',\n",
              " '10 tend much',\n",
              " '10 thank',\n",
              " '10 thank attent',\n",
              " '10 thi case',\n",
              " '10 thi complet',\n",
              " '10 thi could',\n",
              " '10 thi delight',\n",
              " '10 thi effort',\n",
              " '10 thi ha',\n",
              " '10 thi misbegotten',\n",
              " '10 thi truli',\n",
              " '10 thought',\n",
              " '10 thought imdb',\n",
              " '10 time becaus',\n",
              " '10 time better',\n",
              " '10 time betteri',\n",
              " '10 time budget',\n",
              " '10 time cinema',\n",
              " '10 time funnier',\n",
              " '10 time hi',\n",
              " '10 time includ',\n",
              " '10 time past',\n",
              " '10 time sit',\n",
              " '10 time wors',\n",
              " '10 timesth',\n",
              " '10 timesth main',\n",
              " '10 tout',\n",
              " '10 tout 10',\n",
              " '10 tri',\n",
              " '10 tri find',\n",
              " '10 true',\n",
              " '10 true stori',\n",
              " '10 truli',\n",
              " '10 truli one',\n",
              " '10 type',\n",
              " '10 type peopl',\n",
              " '10 underr',\n",
              " '10 underr movi',\n",
              " '10 understand',\n",
              " '10 understand lower',\n",
              " '10 undeserv',\n",
              " '10 undeserv thi',\n",
              " '10 unintent',\n",
              " '10 unintent quick',\n",
              " '10 unwatch',\n",
              " '10 upmi',\n",
              " '10 upmi rate',\n",
              " '10 us',\n",
              " '10 us take',\n",
              " '10 veri',\n",
              " '10 veri harm',\n",
              " '10 vote',\n",
              " '10 vote hi',\n",
              " '10 wa becaus',\n",
              " '10 wa great',\n",
              " '10 wa option',\n",
              " '10 wa still',\n",
              " '10 wa surpris',\n",
              " '10 want',\n",
              " '10 want see',\n",
              " '10 wasnt',\n",
              " '10 wasnt worth',\n",
              " '10 wast',\n",
              " '10 wast time',\n",
              " '10 watch',\n",
              " '10 watch second',\n",
              " '10 well',\n",
              " '10 well done',\n",
              " '10 wetsuit',\n",
              " '10 wetsuit scene',\n",
              " '10 whi',\n",
              " '10 whi isnt',\n",
              " '10 whole',\n",
              " '10 whole film',\n",
              " '10 wish',\n",
              " '10 wish wa',\n",
              " '10 wonder',\n",
              " '10 wonder hollywood',\n",
              " '10 word',\n",
              " '10 word less',\n",
              " '10 work',\n",
              " '10 work revers',\n",
              " '10 worm day',\n",
              " '10 worm els',\n",
              " '10 worm one',\n",
              " '10 worst act',\n",
              " '10 worst movi',\n",
              " '10 would award',\n",
              " '10 would go',\n",
              " '10 would like',\n",
              " '10 would made',\n",
              " '10 would place',\n",
              " '10 would see',\n",
              " '10 year agoid',\n",
              " '10 year behind',\n",
              " '10 year cant',\n",
              " '10 year chili',\n",
              " '10 year climax',\n",
              " '10 year dure',\n",
              " '10 year earlier',\n",
              " '10 year earlieri',\n",
              " '10 year fiercest',\n",
              " '10 year god',\n",
              " '10 year got',\n",
              " '10 year hell',\n",
              " '10 year laugh',\n",
              " '10 year liliom',\n",
              " '10 year mean',\n",
              " '10 year piec',\n",
              " '10 year sinc',\n",
              " '10 year still',\n",
              " '10 year tast',\n",
              " '10 year thi',\n",
              " '10 year think',\n",
              " '10 year time',\n",
              " '10 year would',\n",
              " '10 yearold',\n",
              " '10 yearold boy',\n",
              " '10 yecch',\n",
              " '10 youd',\n",
              " '10 youd give',\n",
              " '10 yr',\n",
              " '10 yr sinc',\n",
              " '100 0f',\n",
              " '100 0f 10',\n",
              " '100 accur difficult',\n",
              " '100 accur film',\n",
              " '100 accur littl',\n",
              " '100 accur revel',\n",
              " '100 adrenalin',\n",
              " '100 adrenalin shock',\n",
              " '100 ani',\n",
              " '100 ani horror',\n",
              " '100 appear',\n",
              " '100 appear baldwin',\n",
              " '100 behind',\n",
              " '100 behind attempt',\n",
              " '100 believ',\n",
              " '100 believ transport',\n",
              " '100 bill',\n",
              " '100 bill thi',\n",
              " '100 bin',\n",
              " '100 bin walmart',\n",
              " '100 bizarr',\n",
              " '100 bizarr humor',\n",
              " '100 black',\n",
              " '100 black white',\n",
              " '100 blue',\n",
              " '100 blue soul',\n",
              " '100 buster',\n",
              " '100 buster keaton',\n",
              " '100 channelssigh',\n",
              " '100 channelssigh ye',\n",
              " '100 charact',\n",
              " '100 charact believ',\n",
              " '100 coher',\n",
              " '100 coher plausibl',\n",
              " '100 comedi',\n",
              " '100 comedi laugh',\n",
              " '100 comedi must',\n",
              " '100 complex',\n",
              " '100 complex android',\n",
              " '100 control',\n",
              " '100 control sub',\n",
              " '100 convinc',\n",
              " '100 convinc christi',\n",
              " '100 correct',\n",
              " '100 correct product',\n",
              " '100 cost',\n",
              " '100 cost rent',\n",
              " '100 costarican',\n",
              " '100 costarican product',\n",
              " '100 degre',\n",
              " '100 degre heat',\n",
              " '100 episod',\n",
              " '100 episod august',\n",
              " '100 everi',\n",
              " '100 everi minut',\n",
              " '100 exampl',\n",
              " '100 exampl see',\n",
              " '100 excel',\n",
              " '100 excel hi',\n",
              " '100 factual',\n",
              " '100 factual documentari',\n",
              " '100 faith',\n",
              " '100 faith book',\n",
              " '100 fake',\n",
              " '100 fake poorli',\n",
              " '100 familyfriendli',\n",
              " '100 familyfriendli someth',\n",
              " '100 feet asternth',\n",
              " '100 feet thi',\n",
              " '100 feet water',\n",
              " '100 film british',\n",
              " '100 film ever',\n",
              " '100 film remak',\n",
              " '100 film timeif',\n",
              " '100 fluent',\n",
              " '100 fluent english',\n",
              " '100 fun',\n",
              " '100 gallon',\n",
              " '100 gallon dalmationprint',\n",
              " '100 give',\n",
              " '100 give thi',\n",
              " '100 gold',\n",
              " '100 gold bad',\n",
              " '100 greatest album',\n",
              " '100 greatest famili',\n",
              " '100 greatest film',\n",
              " '100 greatest pop',\n",
              " '100 greatest scari',\n",
              " '100 greatest tearjerk',\n",
              " '100 greatest war',\n",
              " '100 handmad',\n",
              " '100 handmad shockingli',\n",
              " '100 hiroshima',\n",
              " '100 hiroshima outskirt',\n",
              " '100 imdb 10in',\n",
              " '100 imdba',\n",
              " '100 imdba list',\n",
              " '100 japanes',\n",
              " '100 japanes make',\n",
              " '100 leav',\n",
              " '100 leav less',\n",
              " '100 list',\n",
              " '100 list wouldnt',\n",
              " '100 love',\n",
              " '100 love music',\n",
              " '100 maniac',\n",
              " '100 maniac fight',\n",
              " '100 materi',\n",
              " '100 materi talk',\n",
              " '100 mere',\n",
              " '100 mere mortal',\n",
              " '100 mess',\n",
              " '100 mess kid',\n",
              " '100 mile awayse',\n",
              " '100 mile hour',\n",
              " '100 million dollar',\n",
              " '100 million need',\n",
              " '100 min',\n",
              " '100 min pg',\n",
              " '100 mindless',\n",
              " '100 mindless mental',\n",
              " '100 minut far',\n",
              " '100 minut film',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaFCRJuBfC-A"
      },
      "source": [
        "#### Term Frequency-Inverse Frequency Model (TFIdf)\n",
        "\n",
        "It is used to convert text documents to matrix of tfidf features.\n",
        "\n",
        "The ```tf-idf``` value increases in proportion to the number of times a word appears in the document but is often offset by the frequency of the word in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsceYgJ4e8d5",
        "outputId": "96ddf9ef-3a42-4f6e-c26d-aab57f050899"
      },
      "source": [
        "# TFIdf Vectorizer\n",
        "tfv = TfidfVectorizer(min_df=0, max_df=1, use_idf=True, ngram_range=(1, 3))\n",
        "\n",
        "# Transformed Train and Test reviews\n",
        "tfv_train_reviews = tfv.fit_transform(norm_train_reviews)\n",
        "tfv_test_reviews = tfv.fit_transform(norm_test_reviews)\n",
        "\n",
        "print('Tfidf_train: ', tfv_train_reviews.shape)\n",
        "print('Tfidf_test: ', tfv_test_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tfidf_train:  (40000, 6209089)\n",
            "Tfidf_test:  (10000, 1828357)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy25w0lFgkXS"
      },
      "source": [
        "### Labeling the Sentiment Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnxcOKgbgBR8",
        "outputId": "b528ba66-d1c5-45e6-9afb-286d80bcfe40"
      },
      "source": [
        "lb = LabelBinarizer()\n",
        "\n",
        "# Transformed Sentiment Data\n",
        "sentiment_data = lb.fit_transform(data['sentiment'])\n",
        "print(sentiment_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avu948ECg5WV"
      },
      "source": [
        "### Split the Sentiment Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkvogp_Tg1lN",
        "outputId": "bdec02a6-ba7a-426d-a733-9a102d8acdaa"
      },
      "source": [
        "# Splitting the data\n",
        "train_sentiments = sentiment_data[:40000]\n",
        "test_sentiments = sentiment_data[40000:]\n",
        "\n",
        "print(train_sentiments)\n",
        "print(test_sentiments) ## Done!!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTF4e-FrhNSn"
      },
      "source": [
        "### Modeling the Dataset\n",
        "\n",
        "Build a Logistic Regression Model for both ```bag of words``` and ```Tfidf``` features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaUwfoYYhHRk",
        "outputId": "9d12a451-4618-41d2-f556-116458df85bd"
      },
      "source": [
        "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n",
        "\n",
        "# Fitting the model for Bag of Words\n",
        "lr_bow = lr.fit(cv_train_reviews, train_sentiments)\n",
        "print(lr_bow)\n",
        "\n",
        "# Fitting the model for Tfidf features\n",
        "lr_tfidf = lr.fit(tfv_train_reviews, train_sentiments)\n",
        "print(lr_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTUKrTU6iNao"
      },
      "source": [
        "#### <b>Logistic Regression Model Performance</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "IzOyHMdvh3_O",
        "outputId": "4539ce84-d9b3-4ac5-dfd4-871f1ef7373b"
      },
      "source": [
        "# Predicting for Bag of Words\n",
        "lr_bow_predict = lr_bow.predict(cv_test_reviews)\n",
        "print(lr_bow_predict)\n",
        "\n",
        "# Predicting for TFidf features\n",
        "lr_tfidf_predict = lr_tfidf.predict(tfv_test_reviews)\n",
        "print(lr_tfidf_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b204780f744a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predicting for Bag of Words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr_bow_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_test_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_bow_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Predicting for TFidf features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 273\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
            "\u001b[0;31mValueError\u001b[0m: X has 1828357 features per sample; expecting 6209089"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdDb-Q-RipD7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}